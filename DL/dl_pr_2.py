# -*- coding: utf-8 -*-
"""DL_PR_2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FZ_B1tepiISfzq9cHl322sBY60svGAF2
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Step 1: Load and preprocess the data
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data"
column_names = ["Letter", "X-box", "Y-box", "Width", "Height", "Onpix", "X-bar", "Y-bar", "X2bar", "Y2bar", "XYbar", "X2Ybr", "XY2br", "X-ege", "Xegvy", "Y-ege", "Yegvx"]
data = pd.read_csv(url, header=None, names=column_names)

# Convert letter labels to numerical values
label_encoder = LabelEncoder()
data["Letter"] = label_encoder.fit_transform(data["Letter"])

# Split features and labels
X = data.drop("Letter", axis=1)
y = data["Letter"]

# Normalize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 3: Define the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation="relu", input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dense(26, activation="softmax")  # 26 classes for letters A-Z
])

# Step 4: Compile the model
model.compile(loss="sparse_categorical_crossentropy",
              optimizer="adam",
              metrics=["accuracy"])

# Step 5: Train the model
history = model.fit(X_train, y_train, epochs=20, validation_split=0.2)

# Step 6: Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Test accuracy:", test_accuracy)

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Step 1: Load and preprocess the data
num_words = 10000  # Vocabulary size (top most frequent words to consider)
max_len = 200  # Maximum sequence length for padding

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# Step 3: Define the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(input_dim=num_words, output_dim=16, input_length=max_len),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Step 4: Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 6: Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Test accuracy:", test_accuracy)